# Natural language processing course: `Analysis and Comparison of Translation Errors and Biases in LLMs`

1 ABOUT OUR PROJECT

Our project investigates gender bias in Large Language Models (LLMs) when translating from English to Slovene, focusing on how ambiguous pronouns are translated. English and Slovene have different gender marker structures, and we aim to analyze how LLMs may introduce or reinforce gender biases in these translations.
We will use the WinoBias dataset, which contains gender-biased sentences, to study how LLMs handle pronouns in ambiguous contexts. The analysis will involve one open-source model and one closed-source model.
The purpose of our project is to showcase how LLMs handle gendered language, which is curcial factor in ensuring fairer, less biased translations in multilingual contexts.




2 PROJECT MEMBERS

Pia Polutnik

Tajda Hladnik

Marko Stoklas 




3 REFERENCES

Roberto Navigli, Simone Conia, and Bj√∂rn Ross. 2023. Biases in Large Language Models: Origins, Inventory, and Discussion. J. Data and Information Quality 15, 2, Article 10 (June 2023).
https://doi.org/10.1145/3597307

Barclay, P. J., & Sami, A. (2024). Investigating Markers and Drivers of Gender Bias in Machine Translations. arXiv.Org, abs/2403.11896. 
https://doi.org/10.48550/arxiv.2403.11896

Hadas Kotek, Rikker Dockum, David Sun. 2023. Gender bias and stereotypes in Large Language Models. 
https://dl.acm.org/doi/abs/10.1145/3582269.3615599



**Update on the project 2.5.2025**
We have begun researching how LLMs deal with translation from a language with fewer gender markers to those with a highly gendered language. We added the excell sheet in which we are doing our research, which still looks a bit messy, but will gradually get more tidy over time. In the secondsubmission.pdf we added methodology we used when analyising the close-sourced LLM - ChatGPT and some key findings. More to come in the following week.
